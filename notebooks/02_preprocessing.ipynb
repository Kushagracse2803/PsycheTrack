{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fc08cf",
   "metadata": {},
   "source": [
    "### ðŸ§¹ Preprocessing Steps (in order):\n",
    "\n",
    "Convert text â†’ lowercase âœ…\n",
    "\n",
    "Remove URLs (http... / www...)\n",
    "\n",
    "Remove mentions (@username)\n",
    "\n",
    "Remove hashtags (#depression â†’ depression)\n",
    "\n",
    "Remove numbers & punctuation\n",
    "\n",
    "Remove stopwords (like \"the\", \"is\", \"and\")\n",
    "\n",
    "Lemmatization (running â†’ run, better â†’ good)\n",
    "\n",
    "Save cleaned dataset â†’ data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0605a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#NLTK resource(for stopwords+ Lemmatization Later)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360f018",
   "metadata": {},
   "source": [
    "Step 1: Lowercase Conversion\n",
    "Sabhi letters ko lowercase mein convert karenge:\n",
    "\n",
    "\"yaar, ye movie bahut mast hai! i loved the acting.\"\n",
    "\n",
    "Step 2: Remove Punctuation\n",
    "Comma, exclamation mark jaise punctuation ko hataenge:\n",
    "\n",
    "\"yaar ye movie bahut mast hai i loved the acting\"\n",
    "\n",
    "Step 3: Tokenization\n",
    "Poore sentence ko alag-alag words (tokens) mein tod denge:\n",
    "\n",
    "[\"yaar\", \"ye\", \"movie\", \"bahut\", \"mast\", \"hai\", \"i\", \"loved\", \"the\", \"acting\"]\n",
    "\n",
    "Step 4: Stopwords Removal\n",
    "Stopwords (jaise \"ye\", \"the\", \"hai\", \"i\" etc.) hata denge:\n",
    "\n",
    "[\"yaar\", \"movie\", \"bahut\", \"mast\", \"loved\", \"acting\"]\n",
    "\n",
    "Step 5: Lemmatization/Stemming\n",
    "Words ko unke basic root form mein convert karenge:\n",
    "\n",
    "[\"yaar\", \"movie\", \"bahut\", \"mast\", \"love\", \"act\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae33c3",
   "metadata": {},
   "source": [
    "Stemming sirf suffix hataata hai, lemmatization meaning aur context dekh kar sahi base word laata hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a631f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded dataset: (20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Itâ€™s hard to say whether packing lists are mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           post_text  label\n",
       "0  It's just over 2 years since I was diagnosed w...      1\n",
       "1  It's Sunday, I need a break, so I'm planning t...      1\n",
       "2  Awake but tired. I need to sleep but my brain ...      1\n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...      1\n",
       "4  Itâ€™s hard to say whether packing lists are mak...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the snapshot saved in Notebook 1\n",
    "df = pd.read_csv(\"data/processed/twitter_depression_eda_snapshot.csv\")\n",
    "\n",
    "print(\"Shape of loaded dataset:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58718620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will apply the following preprocessing steps:\n",
      "1. Lowercase text\n",
      "2. Remove URLs\n",
      "3. Remove mentions (@username)\n",
      "4. Remove hashtags (#depression â†’ depression)\n",
      "5. Remove numbers & punctuation\n",
      "6. Remove stopwords\n",
      "7. Lemmatization\n"
     ]
    }
   ],
   "source": [
    "print(\"We will apply the following preprocessing steps:\")\n",
    "steps = [\n",
    "    \"1. Lowercase text\",\n",
    "    \"2. Remove URLs\",\n",
    "    \"3. Remove mentions (@username)\",\n",
    "    \"4. Remove hashtags (#depression â†’ depression)\",\n",
    "    \"5. Remove numbers & punctuation\",\n",
    "    \"6. Remove stopwords\",\n",
    "    \"7. Lemmatization\"\n",
    "]\n",
    "for s in steps:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9788e",
   "metadata": {},
   "source": [
    "### STEP 1:Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8593180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  it's just over 2 years since i was diagnosed w...      1\n",
      "1  it's sunday, i need a break, so i'm planning t...      1\n",
      "2  awake but tired. i need to sleep but my brain ...      1\n",
      "3  rt @sewhq: #retro bears make perfect gifts and...      1\n",
      "4  itâ€™s hard to say whether packing lists are mak...      1\n",
      "5  making packing lists is my new hobby... #movin...      1\n",
      "6  at what point does keeping stuff for nostalgic...      1\n",
      "7  currently in the finding-boxes-of-random-shit ...      1\n",
      "8  can't be bothered to cook, take away on the wa...      1\n",
      "9  rt @itventsnews: itv releases promo video for ...      1\n"
     ]
    }
   ],
   "source": [
    "df['post_text']=df['post_text'].str.lower()\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69726ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['post_text']=df['post_text'].str.replace(r'http\\s+|www\\S+','',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c13714",
   "metadata": {},
   "source": [
    "http\\S+ â†’ match karega \"http\" se start hone wali string (space aane tak).\n",
    "\n",
    "www\\S+ â†’ match karega \"www\" se start hone wali string (space aane tak).\n",
    "\n",
    ".str.replace(..., '', regex=True) â†’ jo bhi match hoga, usko empty string '' se replace kar dega (basically remove kar dega)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42728aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step3: Remove mention @username\n",
    "df['post_text']=df['post_text'].str.replace(r'@\\w+','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3cac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  it's just over 2 years since i was diagnosed w...      1\n",
      "1  it's sunday, i need a break, so i'm planning t...      1\n",
      "2  awake but tired. i need to sleep but my brain ...      1\n",
      "3  rt : #retro bears make perfect gifts and are g...      1\n",
      "4  itâ€™s hard to say whether packing lists are mak...      1\n",
      "5  making packing lists is my new hobby... #movin...      1\n",
      "6  at what point does keeping stuff for nostalgic...      1\n",
      "7  currently in the finding-boxes-of-random-shit ...      1\n",
      "8  can't be bothered to cook, take away on the wa...      1\n",
      "9  rt : itv releases promo video for the final se...      1\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13c2daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  it's just over 2 years since i was diagnosed w...      1\n",
      "1  it's sunday, i need a break, so i'm planning t...      1\n",
      "2  awake but tired. i need to sleep but my brain ...      1\n",
      "3  rt : retro bears make perfect gifts and are gr...      1\n",
      "4  itâ€™s hard to say whether packing lists are mak...      1\n",
      "5  making packing lists is my new hobby... moving...      1\n",
      "6  at what point does keeping stuff for nostalgic...      1\n",
      "7  currently in the finding-boxes-of-random-shit ...      1\n",
      "8  can't be bothered to cook, take away on the wa...      1\n",
      "9  rt : itv releases promo video for the final se...      1\n"
     ]
    }
   ],
   "source": [
    "df['post_text']=df['post_text'].str.replace(r'#','',regex=True)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c35f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  its just over  years since i was diagnosed wit...      1\n",
      "1  its sunday i need a break so im planning to sp...      1\n",
      "2  awake but tired i need to sleep but my brain h...      1\n",
      "3  rt  retro bears make perfect gifts and are gre...      1\n",
      "4  its hard to say whether packing lists are maki...      1\n",
      "5   making packing lists is my new hobby movinghouse      1\n",
      "6  at what point does keeping stuff for nostalgic...      1\n",
      "7  currently in the findingboxesofrandomshit pack...      1\n",
      "8  cant be bothered to cook take away on the way ...      1\n",
      "9  rt  itv releases promo video for the final ser...      1\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Remove numbers and punctuation, keep only letters and spaces\n",
    "df['post_text'] = df['post_text'].str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "\n",
    "# Check a few rows to confirm\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f873239",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60dee38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b46bf768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54af0b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  years since diagnosed anxiety depression today...      1\n",
      "1  sunday need break im planning spend little tim...      1\n",
      "2                 awake tired need sleep brain ideas      1\n",
      "3  rt retro bears make perfect gifts great beginn...      1\n",
      "4  hard say whether packing lists making life eas...      1\n",
      "5         making packing lists new hobby movinghouse      1\n",
      "6  point keeping stuff nostalgic reasons cross li...      1\n",
      "7  currently findingboxesofrandomshit packing pha...      1\n",
      "8              cant bothered cook take away way lazy      1\n",
      "9  rt itv releases promo video final series downt...      1\n"
     ]
    }
   ],
   "source": [
    "df['post_text']=df['post_text'].apply(remove_stopwords)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707eca2b",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a0d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70efa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "858e658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           post_text  label\n",
      "0  year since diagnosed anxiety depression today ...      1\n",
      "1  sunday need break im planning spend little tim...      1\n",
      "2                  awake tired need sleep brain idea      1\n",
      "3  rt retro bear make perfect gift great beginner...      1\n",
      "4  hard say whether packing list making life easi...      1\n",
      "5          making packing list new hobby movinghouse      1\n",
      "6  point keeping stuff nostalgic reason cross lin...      1\n",
      "7  currently findingboxesofrandomshit packing pha...      1\n",
      "8              cant bothered cook take away way lazy      1\n",
      "9  rt itv release promo video final series downto...      1\n"
     ]
    }
   ],
   "source": [
    "## FUNTION TO LEMMATIZE EACH WORD\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "## Apply on dataset\n",
    "df['post_text'] =df['post_text'].apply(lemmatize_text)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003cfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE DATA SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48525392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned dataset saved at: data/processed/twitter_depression_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data/processed\",exist_ok=True)\n",
    "\n",
    "#Save cleaned data det\n",
    "df.to_csv(\"data/processed/twitter_depression_cleaned.csv\",index=False)\n",
    "print(\"âœ… Cleaned dataset saved at: data/processed/twitter_depression_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2f472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
